{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pegasus-code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9EQL_eVovPH",
        "outputId": "9582d85b-d1df-4198-9ea0-1b52255cde02"
      },
      "source": [
        "'''\r\n",
        "\thttps://github.com/jzbontar/pixelcnn-pytorch\r\n",
        "    https://github.com/singh-hrituraj/PixelCNN-Pytorch/blob/master/MaskedCNN.py\r\n",
        "    https://github.com/singh-hrituraj/PixelCNN-Pytorch/blob/master/Model.py\r\n",
        "    https://github.com/singh-hrituraj/PixelCNN-Pytorch/blob/master/train.py\r\n",
        "    https://github.com/singh-hrituraj/PixelCNN-Pytorch/blob/master/generate.py\r\n",
        "    https://stackoverflow.com/questions/65172786/how-to-load-one-type-of-image-in-cifar10-or-stl10-with-pytorch\r\n",
        "    https://github.com/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb\r\n",
        "    Have left my code as it was when I last ran it on stl 96x96, requires changes to functions to run on cifar10 though.\r\n",
        "    Also not mentioned in my report added gated activation functions for pixelcnn\r\n",
        "'''\r\n",
        "\r\n",
        "import math\r\n",
        "import random\r\n",
        "from os.path import exists\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "from torch.nn.utils import weight_norm as wn\r\n",
        "#from matplotlib.animation import FuncAnimation\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "manualSeed = np.random.randint(1, 10000)\r\n",
        "print(\"Random Seed: \", manualSeed)\r\n",
        "np.random.seed(manualSeed)\r\n",
        "torch.manual_seed(manualSeed)\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "def cycle(iterable):\r\n",
        "    while True:\r\n",
        "        for x in iterable:\r\n",
        "            yield x\r\n",
        "\r\n",
        "def stl_classes(data_set, label):\r\n",
        "    indices = []\r\n",
        "    for i in range(len(data_set)):\r\n",
        "        if data_set[i][1] == label:\r\n",
        "            indices.append(i)\r\n",
        "    return indices\r\n",
        "\r\n",
        "def get_data(dataset):\r\n",
        "    trans = torchvision.transforms.Compose([\r\n",
        "                                    torchvision.transforms.ToTensor(),\r\n",
        "                                    torchvision.transforms.Resize((img_dim, img_dim)),\r\n",
        "                                    torchvision.transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\r\n",
        "                                ])\r\n",
        "\r\n",
        "    if 'cifar10' == dataset:\r\n",
        "        train_data_set = torchvision.datasets.CIFAR10('drive/My Drive/training/cifar10', train=True, download=True, transform=trans)\r\n",
        "        \r\n",
        "        indices_birds = torch.tensor(train_data_set.targets) == 2\r\n",
        "        indices_sub_birds = [366, 443, 613, 626, 765, 1126, 1171, 2063, 2250, 2372, 2490, 3093, 3139, 3248, 3408, 3448, 4722]#[154, 200, 228, 558, 567, 591, 738, 782, 999, 1016, 1033, 1037, 1095, 1126, 1131, 1176, 1408, 1424, 1443, 1476, 1560, 1581, 1705, 1718, 1736, 1835, 1901, 2013, 2098, 2106, 2157, 2243, 2250, 2405, 2438, 2489, 2505, 2588, 2595, 2653, 2673, 2682, 2722, 2918, 3072, 3139, 2158, 3248, 3319, 3335, 3339, 3351, 3352, 3418, 3448, 3486, 4722]\r\n",
        "\r\n",
        "        indices_horses = torch.tensor(train_data_set.targets) == 7\r\n",
        "        indices_sub_horses = [6, 15, 19, 30, 86, 106, 107, 114, 177, 210, 228, 237, 292, 315, 353, 354, 502, 527, 547, 548, 656, 470, 670, 799, 800, 805, 814, 816, 823, 903, 910, 934, 948, 992, 993, 1013, 1066, 1076, 1103, 1124, 1191, 1201, 1251, 1313, 1340, 1343, 1384, 1439, 1463, 1485, 1528, 1529, 1574, 1677, 1772]\r\n",
        "\r\n",
        "        print(len(indices_sub_birds), len(indices_sub_horses))\r\n",
        "        horse_data_set = torch.utils.data.dataset.Subset(train_data_set, np.where(indices_horses==1)[0])\r\n",
        "        horse_data_set = torch.utils.data.dataset.Subset(horse_data_set, indices_sub_horses)\r\n",
        "\r\n",
        "        bird_data_set = torch.utils.data.dataset.Subset(train_data_set, np.where(indices_birds==1)[0])\r\n",
        "        bird_data_set = torch.utils.data.dataset.Subset(bird_data_set, indices_sub_birds)\r\n",
        "\r\n",
        "        horse_loader = torch.utils.data.DataLoader(horse_data_set, \r\n",
        "            shuffle=False, batch_size=1, drop_last=True, pin_memory=True)\r\n",
        "        bird_loader = torch.utils.data.DataLoader(bird_data_set, \r\n",
        "            shuffle=False, batch_size=1, drop_last=True, pin_memory=True)\r\n",
        "        train_loader = torch.utils.data.DataLoader(train_data_set,\r\n",
        "            shuffle=False, batch_size=batch_size, drop_last=True, pin_memory=True)\r\n",
        "        \r\n",
        "        class_names = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\r\n",
        "\r\n",
        "    if 'stl10' == dataset:\r\n",
        "        train_data_set = torchvision.datasets.STL10('drive/My Drive/training/stl10', split='train+unlabeled', download=True, transform=trans)\r\n",
        "        indices_birds = stl_classes(train_data_set, 1)\r\n",
        "        indices_sub_birds = [30, 41, 128, 156, 159, 256, 360]\r\n",
        "        indices_horses = stl_classes(train_data_set, 6)\r\n",
        "        indices_sub_horses = [3, 4, 9, 10, 18, 38, 53, 56, 86, 179, 184, 192, 201, 215, 227, 246, 286, 300, 302, 319, 320]\r\n",
        "\r\n",
        "        bird_data_set = torch.utils.data.dataset.Subset(train_data_set, indices_birds)\r\n",
        "        bird_data_set = torch.utils.data.dataset.Subset(bird_data_set, indices_sub_birds)\r\n",
        "\r\n",
        "        horse_data_set = torch.utils.data.dataset.Subset(train_data_set, indices_horses)\r\n",
        "        horse_data_set = torch.utils.data.dataset.Subset(horse_data_set, indices_sub_horses)\r\n",
        "\r\n",
        "        horse_loader = torch.utils.data.DataLoader(horse_data_set, \r\n",
        "            shuffle=False, batch_size=1, drop_last=True)\r\n",
        "        bird_loader = torch.utils.data.DataLoader(bird_data_set, \r\n",
        "            shuffle=False, batch_size=1, drop_last=True)\r\n",
        "\r\n",
        "        train_loader = torch.utils.data.DataLoader(train_data_set, shuffle=False, batch_size=batch_size, drop_last=True, pin_memory=True)\r\n",
        "        train_iterator = iter(cycle(train_loader))\r\n",
        "        class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck'] # these are slightly different to CIFAR-10\r\n",
        "        \r\n",
        "    \r\n",
        "    '''\r\n",
        "    to_show = iter(cycle(bird_loader))\r\n",
        "    for i in range(17):\r\n",
        "        print(i)\r\n",
        "        img, _ = next(to_show)\r\n",
        "        img = img + 0.5\r\n",
        "        plt.rcParams['figure.dpi'] = 150\r\n",
        "        plt.grid(False)\r\n",
        "        plt.imshow(torchvision.utils.make_grid(img.view(img.size()[0], n_channels, img_dim, img_dim)).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "    to_show = iter(cycle(horse_loader))\r\n",
        "    for i in range(13):\r\n",
        "        print(i)\r\n",
        "        img, _ = next(to_show)\r\n",
        "        img = img + 0.5\r\n",
        "        plt.rcParams['figure.dpi'] = 150\r\n",
        "        plt.grid(False)\r\n",
        "        plt.imshow(torchvision.utils.make_grid(img.view(img.size()[0], n_channels, img_dim, img_dim)).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "        plt.show()\r\n",
        "    '''\r\n",
        "    data_varience = 1.0#np.var(train_data_set.data / 255.0)\r\n",
        "    return train_loader, len(train_loader), class_names, bird_loader, horse_loader, data_varience\r\n",
        "\r\n",
        "class Quantise(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Quantise, self).__init__()\r\n",
        "        self.v_width = 128\r\n",
        "        self.num_v = 256\r\n",
        "        self.mse = lambda x, y: ((x - y)**2).mean()\r\n",
        "        \r\n",
        "        self.e_table = nn.Embedding(self.num_v, self.v_width)\r\n",
        "        self.e_table.weight.data.uniform_(-1/self.num_v, 1/self.num_v)\r\n",
        "        self.beta = 0.25\r\n",
        "\r\n",
        "    def forward(self, z_features):\r\n",
        "        z_features = z_features.permute(0, 2, 3, 1).contiguous()\r\n",
        "        latent_shape = z_features.shape\r\n",
        "        feature_vect = z_features.view(-1, self.v_width)\r\n",
        "\r\n",
        "        \r\n",
        "        distances = (torch.sum(feature_vect**2, dim=1, keepdim=True) \r\n",
        "                    + torch.sum(self.e_table.weight**2, dim=1)\r\n",
        "                    - 2 * torch.matmul(feature_vect, self.e_table.weight.t()))\r\n",
        "            \r\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\r\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.num_v, device=z_features.device)\r\n",
        "        encodings.scatter_(1, encoding_indices, 1)\r\n",
        "        \r\n",
        "        quantized = torch.matmul(encodings, self.e_table.weight).view(latent_shape)\r\n",
        "        encoding_indices = encoding_indices.view(latent_shape[0], latent_shape[1], latent_shape[2], 1)\r\n",
        "        \r\n",
        "        e_latent_loss = self.mse(quantized.detach(), z_features)\r\n",
        "        q_latent_loss = self.mse(quantized, z_features.detach())\r\n",
        "        loss = q_latent_loss + self.beta * e_latent_loss\r\n",
        "        \r\n",
        "        quantized = z_features + (quantized - z_features).detach()\r\n",
        "        avg_probs = torch.mean(encodings, dim=0)\r\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\r\n",
        "        \r\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encoding_indices.permute(0, 3, 1, 2).contiguous()\r\n",
        "\r\n",
        "class EncodeBlock(nn.Module):\r\n",
        "    def __init__(self, in_f, out_f, k_size, stride, padding=0):\r\n",
        "        super(EncodeBlock, self).__init__()\r\n",
        "        self.f_pass = nn.Sequential(\r\n",
        "            nn.Conv2d(in_f, out_f, kernel_size=k_size, stride=stride, padding=padding),\r\n",
        "            #nn.BatchNorm2d(out_f),\r\n",
        "            nn.ReLU(True)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        return self.f_pass(x)\r\n",
        "\r\n",
        "class Residual(nn.Module):\r\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\r\n",
        "        super(Residual, self).__init__()\r\n",
        "        self._block = nn.Sequential(\r\n",
        "            nn.ReLU(True),\r\n",
        "            nn.Conv2d(in_channels=in_channels,\r\n",
        "                      out_channels=num_residual_hiddens,\r\n",
        "                      kernel_size=3, stride=1, padding=1, bias=False),\r\n",
        "            nn.ReLU(True),\r\n",
        "            nn.Conv2d(in_channels=num_residual_hiddens,\r\n",
        "                      out_channels=num_hiddens,\r\n",
        "                      kernel_size=1, stride=1, bias=False)\r\n",
        "        )\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        return x + self._block(x)\r\n",
        "\r\n",
        "\r\n",
        "class ResidualStack(nn.Module):\r\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\r\n",
        "        super(ResidualStack, self).__init__()\r\n",
        "        self._num_residual_layers = num_residual_layers\r\n",
        "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\r\n",
        "                             for _ in range(self._num_residual_layers)])\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        for i in range(self._num_residual_layers):\r\n",
        "            x = self._layers[i](x)\r\n",
        "        return F.relu(x) \r\n",
        "\r\n",
        "class DecodeBlock(nn.Module):\r\n",
        "    def __init__(self, in_f, out_f, k_size, stride, padding=0):\r\n",
        "        super(DecodeBlock, self).__init__()\r\n",
        "        self.f_pass = nn.Sequential(\r\n",
        "            nn.ConvTranspose2d(in_f, out_f, kernel_size=k_size, stride=stride, padding=padding),\r\n",
        "            #nn.BatchNorm2d(out_f),\r\n",
        "            nn.ReLU(True)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        return self.f_pass(x) \r\n",
        "\r\n",
        "class MaskedCNN(nn.Conv2d):\r\n",
        "\tdef __init__(self, mask_type, *args, **kwargs):\r\n",
        "\t\tself.mask_type = mask_type\r\n",
        "\t\tassert mask_type in ['A', 'B'], \"Unknown Mask Type\"\r\n",
        "\t\tsuper(MaskedCNN, self).__init__(*args, **kwargs)\r\n",
        "\t\tself.register_buffer('mask', self.weight.data.clone())\r\n",
        "\r\n",
        "\t\t_, depth, height, width = self.weight.size()\r\n",
        "\t\tself.mask.fill_(1)\r\n",
        "\t\tif mask_type =='A':\r\n",
        "\t\t\tself.mask[:,:,height//2,width//2:] = 0\r\n",
        "\t\t\tself.mask[:,:,height//2+1:,:] = 0\r\n",
        "\t\telse:\r\n",
        "\t\t\tself.mask[:,:,height//2,width//2+1:] = 0\r\n",
        "\t\t\tself.mask[:,:,height//2+1:,:] = 0\r\n",
        "\r\n",
        "\tdef forward(self, x):\r\n",
        "\t\tself.weight.data*=self.mask\r\n",
        "\t\treturn super(MaskedCNN, self).forward(x)\r\n",
        "\r\n",
        "class Gated_Act(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Gated_Act, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return torch.tanh(x) * torch.sigmoid(x)\r\n",
        "\r\n",
        "class PixelBlock(nn.Module):\r\n",
        "    def __init__(self, mask_type, in_f, out_f, k_size):\r\n",
        "        super(PixelBlock, self).__init__()\r\n",
        "        self.f_pass = nn.Sequential(\r\n",
        "            MaskedCNN(mask_type, in_f, out_f, k_size, 1, k_size//2, bias=False),\r\n",
        "            nn.BatchNorm2d(out_f),\r\n",
        "            Gated_Act()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        return self.f_pass(x) \r\n",
        "\r\n",
        "class PixelCNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(PixelCNN, self).__init__()\r\n",
        "        filters = 128\r\n",
        "        k_size = 7\r\n",
        "        self.main = nn.Sequential(\r\n",
        "            PixelBlock('A', 1, filters, k_size),\r\n",
        "            PixelBlock('B', filters, filters, k_size),\r\n",
        "            PixelBlock('B', filters, filters, k_size),\r\n",
        "            PixelBlock('B', filters, filters, k_size),\r\n",
        "            PixelBlock('B', filters, filters, k_size),\r\n",
        "            PixelBlock('B', filters, filters, k_size),\r\n",
        "            PixelBlock('B', filters, filters, k_size),\r\n",
        "            PixelBlock('B', filters, filters, k_size),\r\n",
        "            nn.Conv2d(filters, 256, 1)\r\n",
        "        )\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        return self.main(x)\r\n",
        "\r\n",
        "def show_latent(image1, image2):\r\n",
        "    print(\"lat\")\r\n",
        "    plt.rcParams['figure.dpi'] = 101\r\n",
        "    plt.imshow(torchvision.utils.make_grid(image1.view(batch_size, 1, 8, 8)[:32]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "    plt.show()   \r\n",
        "    plt.rcParams['figure.dpi'] = 101\r\n",
        "    plt.imshow(torchvision.utils.make_grid(image2.view(batch_size, 1, 8, 8)[:32]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "    plt.show()   \r\n",
        "\r\n",
        "def show_quantised(image1, image2):\r\n",
        "    print(\"quan\")\r\n",
        "    plt.rcParams['figure.dpi'] = 101\r\n",
        "    plt.imshow(torchvision.utils.make_grid(image1[:,0:3].view(batch_size, 3, 8, 8)[:32]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "    plt.show()   \r\n",
        "    plt.rcParams['figure.dpi'] = 101\r\n",
        "    plt.imshow(torchvision.utils.make_grid(image2[:,0:3].view(batch_size, 3, 8, 8)[:32]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "    plt.show()   \r\n",
        "\r\n",
        "class VQ_VAE(nn.Module):\r\n",
        "    def __init__(self, hidden_dim=1024, latent_dim=32, latent_channels = 128):\r\n",
        "        super(VQ_VAE, self).__init__()\r\n",
        "        #Input to encode should be (64, 3, 32, 32) -> maybe should experiment with max pooling here\r\n",
        "        self.mse = lambda x, y: ((x - y)**2).mean()\r\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='sum')\r\n",
        "\r\n",
        "        self.quantise = Quantise()\r\n",
        "\r\n",
        "        self.encode = nn.Sequential(\r\n",
        "            EncodeBlock(n_channels, latent_channels//2, 4, 2, 1),\r\n",
        "            EncodeBlock(latent_channels//2, latent_channels, 4, 2, 1),\r\n",
        "            EncodeBlock(latent_channels, latent_channels, 4, 2, 1),\r\n",
        "            nn.Conv2d(in_channels=latent_channels, out_channels=latent_channels,kernel_size=3,stride=1, padding=1),\r\n",
        "            ResidualStack(in_channels=latent_channels,\r\n",
        "                        num_hiddens=latent_channels,\r\n",
        "                        num_residual_layers=2,\r\n",
        "                        num_residual_hiddens=32),\r\n",
        "            nn.Conv2d(latent_channels, self.quantise.v_width, kernel_size=1, stride=1),\r\n",
        "        )\r\n",
        "\r\n",
        "        self.decode = nn.Sequential(\r\n",
        "            nn.Conv2d(self.quantise.v_width, latent_channels, kernel_size=3, stride=1, padding=1),\r\n",
        "            ResidualStack(in_channels=latent_channels,\r\n",
        "                        num_hiddens=latent_channels,\r\n",
        "                        num_residual_layers=2,\r\n",
        "                        num_residual_hiddens=32),\r\n",
        "            DecodeBlock(latent_channels, latent_channels, 4, 2, 1),\r\n",
        "            DecodeBlock(latent_channels, latent_channels//2, 4, 2, 1),\r\n",
        "            nn.ConvTranspose2d(in_channels=latent_channels//2, out_channels=n_channels, kernel_size=4, stride=2, padding=1)\r\n",
        "        )                    \r\n",
        "\r\n",
        "    \r\n",
        "    def interpolate(self, z_x, z_y, per):\r\n",
        "        pers = [per]#[0.11, 0.22, 0.33, 0.44, 0.55, 0.66, 0.77, 0.88]\r\n",
        "        #pers = [0.40 for i in range(z_x.size()[0])]\r\n",
        "        \r\n",
        "        #pers = [0.3 + (i * 0.04) for i in range(8)]\r\n",
        "\r\n",
        "        interpolations = torch.zeros(z_x.size()).to(device)\r\n",
        "        for per in range(z_x.size()[0]):\r\n",
        "            interpolations[per] = (z_x[per] * pers[per]) + (z_y[per] * (1 - pers[per]))\r\n",
        "        \r\n",
        "        return interpolations\r\n",
        "\r\n",
        "    def forward(self, x, detached=False):\r\n",
        "        z = self.encode(x)\r\n",
        "\r\n",
        "        loss, quantised, perplexity, indices = self.quantise(z)\r\n",
        "\r\n",
        "        recon_x = self.decode(quantised)\r\n",
        "        int_x = recon_x\r\n",
        "\r\n",
        "        return loss, recon_x, int_x, indices, quantised\r\n",
        "    \r\n",
        "    def forward_interpolate(self, x, y, per):\r\n",
        "        z_x = self.encode(x)\r\n",
        "        z_y = self.encode(y)\r\n",
        "\r\n",
        "        z_int = self.interpolate(z_x, z_y, per)\r\n",
        "\r\n",
        "        _, quantised_z, _, indices_z = self.quantise(z_int.detach())\r\n",
        "\r\n",
        "        int_x = self.decode(quantised_z)\r\n",
        "\r\n",
        "        return int_x\r\n",
        "\r\n",
        "def decode_sample(sample, model):\r\n",
        "    sample = sample.type(torch.int64)\r\n",
        "    sample = sample.permute(0, 2, 3, 1)\r\n",
        "    encoding_indices = sample.view(-1, 1).to(device)\r\n",
        "    encodings = torch.zeros(encoding_indices.shape[0], model.quantise.num_v, device=device)\r\n",
        "    encodings.scatter_(1, encoding_indices, 1)\r\n",
        "    quantized = torch.matmul(encodings, model.quantise.e_table.weight).view(sample.size()[0], sample.size()[1], sample.size()[2], model.quantise.v_width)\r\n",
        "    quantized = quantized.permute(0, 3, 1, 2).contiguous()\r\n",
        "    return quantized\r\n",
        "\r\n",
        "def fit_VAE(model, data_loader, optimiser):\r\n",
        "    model.train()\r\n",
        "    loss_arr = np.zeros(0)\r\n",
        "    print(len_train)\r\n",
        "\r\n",
        "    for i in range(400):\r\n",
        "        x, _ = next(train_iterator)\r\n",
        "        x = x.to(device)\r\n",
        "        optimiser.zero_grad()\r\n",
        "\r\n",
        "        vq_loss, recon_x, int_x, _, _ = model(x)\r\n",
        "\r\n",
        "        recon_loss = model.mse(recon_x, x) / data_varience\r\n",
        "        loss = recon_loss + vq_loss\r\n",
        "\r\n",
        "        loss.backward() \r\n",
        "        optimiser.step()\r\n",
        "\r\n",
        "        loss_arr = np.append(loss_arr, loss.item()/batch_size)\r\n",
        "    return loss_arr\r\n",
        "\r\n",
        "def validate_VAE(model, data_loader):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    loss_arr = np.zeros(0)\r\n",
        "    with torch.no_grad():\r\n",
        "        for i in range(10):\r\n",
        "            x,t = next(train_iterator)\r\n",
        "            x,t = x.to(device), t.to(device)\r\n",
        "            vq_loss, recon_x, int_x, _, _ = model(x)\r\n",
        "\r\n",
        "            recon_loss = model.mse(recon_x, x) / data_varience\r\n",
        "            loss = recon_loss + vq_loss\r\n",
        "    \r\n",
        "            loss_arr = np.append(loss_arr, loss.item()/batch_size)\r\n",
        "    return loss_arr, x, recon_x, int_x\r\n",
        "\r\n",
        "def fit_Pixel(model, model_p, horse_iterator, bird_iterator, optimiser, optimiser_p, interpolations):\r\n",
        "    model.train()\r\n",
        "    model_p.train()\r\n",
        "\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    loss_arr = np.zeros(0)\r\n",
        "    num_int = interpolations.size()[0]+1\r\n",
        "    print(len_train)\r\n",
        "    for i in range(100):\r\n",
        "        h, _ = next(horse_iterator)\r\n",
        "        h = h.to(device)\r\n",
        "        for i in range(batch_size - num_int):\r\n",
        "            h = torch.cat((h, next(horse_iterator)[0].to(device)))\r\n",
        "        b = interpolations.to(device)\r\n",
        "        comb = torch.cat((h, b), dim=0)\r\n",
        "\r\n",
        "        loss, recon_x, _, indices, quantised = model(comb)\r\n",
        "\r\n",
        "        indices = Variable(indices).to(device) * 1.\r\n",
        "\r\n",
        "        output = model_p(indices) \r\n",
        "        \r\n",
        "        loss = criterion(output, indices[:, 0, :, :].long())\r\n",
        "\r\n",
        "        optimiser_p.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimiser_p.step()\r\n",
        "        \r\n",
        "        loss_arr = np.append(loss_arr, loss.item()/batch_size)\r\n",
        "    return loss_arr\r\n",
        "\r\n",
        "       \r\n",
        "def validate_Pixel(model, model_p, horse_iterator, bird_iterator, interpolations):\r\n",
        "    model.eval()\r\n",
        "    model_p.eval()\r\n",
        "\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    loss_arr = np.zeros(0)\r\n",
        "\r\n",
        "    num_int = interpolations.size()[0]+1\r\n",
        "    with torch.no_grad():\r\n",
        "        for i in range(10):\r\n",
        "            h, _ = next(horse_iterator)\r\n",
        "            h = h.to(device)\r\n",
        "            for i in range(batch_size - num_int):\r\n",
        "                h = torch.cat((h, next(horse_iterator)[0].to(device)))\r\n",
        "            b = interpolations.to(device)\r\n",
        "            comb = torch.cat((h, b), dim=0)\r\n",
        "\r\n",
        "            loss, recon_x, _, indices, quantised = model(comb)\r\n",
        "    \r\n",
        "            indices = Variable(indices).to(device) * 1.\r\n",
        "    \r\n",
        "            output = model_p(indices) \r\n",
        "    \r\n",
        "            loss = criterion(output, indices[:, 0, :, :].long())\r\n",
        "    \r\n",
        "            loss_arr = np.append(loss_arr, loss.item()/batch_size)\r\n",
        "\r\n",
        "        i_size = indices.size()[3] \r\n",
        "        sample = torch.Tensor(batch_size, 1, i_size, i_size).to(device)\r\n",
        "        sample.fill_(0)\r\n",
        "    \r\n",
        "        for i in range(i_size):\r\n",
        "            for j in range(i_size):\r\n",
        "                out = model_p(sample)\r\n",
        "                probs = F.softmax(out[:,:,i,j], dim=-1).data\r\n",
        "                sample[:,:,i,j] = torch.multinomial(probs, 1).int()\r\n",
        "\r\n",
        "        q_latent = decode_sample(sample, model)\r\n",
        "        sample = model.decode(q_latent)\r\n",
        "    return loss_arr, comb, recon_x, sample\r\n",
        "\r\n",
        "#Add plot of latent space in generator\r\n",
        "def show_stats(loss_train, loss_test):\r\n",
        "    fig = plt.figure(figsize=(6, 4))\r\n",
        "    ax = fig.add_subplot(111)\r\n",
        "    plt.plot(np.arange(0, loss_train.size), loss_train)\r\n",
        "    plt.plot(np.arange(0, loss_test.size), loss_test)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def show_example(original, reconstruction, interpolated):\r\n",
        "    plt.rcParams['figure.dpi'] = 101\r\n",
        "    plt.grid(False)\r\n",
        "    plt.imshow(torchvision.utils.make_grid(original.view(batch_size, n_channels, img_dim, img_dim)[:8]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "    plt.show()\r\n",
        "    plt.grid(False)\r\n",
        "    plt.imshow(torchvision.utils.make_grid(reconstruction.view(batch_size, n_channels, img_dim, img_dim)[:8]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "    plt.show()\r\n",
        "    plt.grid(False)\r\n",
        "    plt.imshow(torchvision.utils.make_grid(interpolated.view(batch_size, n_channels, img_dim, img_dim)).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    plt.pause(0.0001)\r\n",
        "\r\n",
        "def show_interpolation(images):\r\n",
        "    plt.rcParams['figure.dpi'] = 101\r\n",
        "    plt.imshow(torchvision.utils.make_grid(images.view(images.size()[0], n_channels, img_dim, img_dim)[:32]).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def show_batch(original, reconstruction, interpolated):\r\n",
        "    plt.rcParams['figure.dpi'] = 150\r\n",
        "    plt.grid(False)\r\n",
        "    plt.imshow(torchvision.utils.make_grid(interpolated).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def set_up_model():\r\n",
        "    epoch = 0\r\n",
        "    model = VQ_VAE().to(device)\r\n",
        "    optimiser = torch.optim.Adam(model.parameters(), lr=VQ_lr)\r\n",
        "\r\n",
        "    epoch_p = 0\r\n",
        "    model_p = PixelCNN().to(device)\r\n",
        "    optimiser_p = torch.optim.Adam(model_p.parameters(), lr=Pixel_lr)\r\n",
        "    scheduler = lr_scheduler.StepLR(optimiser_p, step_size=1, gamma=lr_decay)\r\n",
        "\r\n",
        "    if exists('drive/My Drive/training/VQ-VAE-' + str(img_dim) + '.chkpt'):\r\n",
        "        params_vq = torch.load('drive/My Drive/training/VQ-VAE-' + str(img_dim) + '.chkpt')\r\n",
        "        model.load_state_dict(params_vq['VQVAE'])\r\n",
        "        optimiser.load_state_dict(params_vq['optimiser'])\r\n",
        "        epoch = params_vq['epoch']\r\n",
        "\r\n",
        "    if exists('drive/My Drive/training/PixelCNN-' + str(img_dim) + '.chkpt'):\r\n",
        "        params = torch.load('drive/My Drive/training/PixelCNN-' + str(img_dim) + '.chkpt')\r\n",
        "        model_p.load_state_dict(params['model_p'])\r\n",
        "        scheduler.load_state_dict(params['scheduler'])\r\n",
        "        optimiser_p.load_state_dict(params['optimiser_p'])\r\n",
        "        epoch_p = params['epoch']\r\n",
        "    return model, epoch, optimiser, scheduler, model_p, optimiser_p, epoch_p\r\n",
        "\r\n",
        "def train(model, model_p, epoch, optimiser, optimiser_p, epoch_p):\r\n",
        "    loss_train, loss_test = np.zeros(0), np.zeros(0)\r\n",
        "    loss_g_train, loss_d_train = np.zeros(0), np.zeros(0)\r\n",
        "\r\n",
        "    while (epoch < 40):\r\n",
        "        avg_batch_loss_t = fit_VAE(model, train_loader, optimiser)\r\n",
        "        avg_batch_loss_v, original, reconstruction, interpolated = validate_VAE(model, train_loader)\r\n",
        "\r\n",
        "        print('train_loss ' + str(avg_batch_loss_t.mean()))\r\n",
        "        print('val_loss ' + str(avg_batch_loss_v.mean()))\r\n",
        "\r\n",
        "        loss_train = np.concatenate([loss_train, avg_batch_loss_t])\r\n",
        "        loss_test = np.concatenate([loss_test, avg_batch_loss_v])\r\n",
        "\r\n",
        "        show_stats(loss_train, loss_test)\r\n",
        "        show_stats(avg_batch_loss_t, avg_batch_loss_v)\r\n",
        "        show_example(original+0.5, reconstruction+0.5, interpolated+0.5)\r\n",
        "        print(epoch)\r\n",
        "        torch.save({'VQVAE':model.state_dict(),\r\n",
        "                    'optimiser':optimiser.state_dict(), \r\n",
        "                     'epoch':epoch}, \r\n",
        "                    'drive/My Drive/training/VQ-VAE-' + str(img_dim) + '.chkpt')\r\n",
        "        epoch = epoch+1\r\n",
        "\r\n",
        "    interpolations = interpolate_birds_horses(model)\r\n",
        "    while (epoch_p < 103):\r\n",
        "\r\n",
        "        avg_batch_loss_t = fit_Pixel(model, model_p, horse_iterator, bird_iterator, optimiser, optimiser_p, interpolations)\r\n",
        "        avg_batch_loss_v, original, reconstruction, recon_z = validate_Pixel(model, model_p, horse_iterator, bird_iterator, interpolations)\r\n",
        "\r\n",
        "        print('train_loss ' + str(avg_batch_loss_t.mean()))\r\n",
        "        print('val_loss ' + str(avg_batch_loss_v.mean()))\r\n",
        "\r\n",
        "        loss_train = np.concatenate([loss_train, avg_batch_loss_t])\r\n",
        "        loss_test = np.concatenate([loss_test, avg_batch_loss_v])\r\n",
        "\r\n",
        "        show_stats(loss_train, loss_test)\r\n",
        "        show_stats(avg_batch_loss_t, avg_batch_loss_v)\r\n",
        "\r\n",
        "        show_example(original+0.5, reconstruction+0.5, recon_z+0.5)\r\n",
        "        print(epoch_p)\r\n",
        "        epoch_p = epoch_p+1 \r\n",
        "        torch.save({'model_p':model_p.state_dict(),\r\n",
        "                    'optimiser_p':optimiser_p.state_dict(), \r\n",
        "                    'scheduler':scheduler.state_dict(), \r\n",
        "                    'epoch':epoch_p}, \r\n",
        "                    'drive/My Drive/training/PixelCNN-' + str(img_dim) + '.chkpt')\r\n",
        "    for i in range(250):\r\n",
        "        avg_batch_loss_v, original, reconstruction, recon_z = validate_Pixel(model, model_p, horse_iterator, bird_iterator, interpolations)\r\n",
        "        show_batch(original+0.5, reconstruction+0.5, recon_z+0.5)\r\n",
        "\r\n",
        "\r\n",
        "def interpolate_birds_horses(model):\r\n",
        "    interps = None \r\n",
        "    for hind in range(21):\r\n",
        "        h_org = next(horse_iterator)[0].to(device)\r\n",
        "        for bind in range(7):\r\n",
        "            b = torchvision.transforms.functional.adjust_contrast(next(bird_iterator)[0].to(device), 2)\r\n",
        "            b = torchvision.transforms.functional.adjust_gamma(b, 1.5)\r\n",
        "            for per in [0.66]:\r\n",
        "                if bind == 4:\r\n",
        "                    h = h_org\r\n",
        "                    print(\"h:   \", hind, \"b:   \", bind)\r\n",
        "                    inte1 = model.forward_interpolate(h, b, per)\r\n",
        "                    b = torchvision.transforms.functional.hflip(b)\r\n",
        "                    h = h_org\r\n",
        "                    print(\"h:   \", hind, \"b:   \", bind)\r\n",
        "                    inte2 = model.forward_interpolate(h, b, per)\r\n",
        "    \r\n",
        "                    if hind:\r\n",
        "                        interps = torch.cat((interps, inte1, inte2), dim=0)\r\n",
        "                    else:\r\n",
        "                        interps = torch.cat((inte1, inte2), dim=0)\r\n",
        "    print(interps.size())\r\n",
        "    print(\"------------------------------------------------------ \\n ---------------------------------------------------------- \\n -------------------------------------------------\")\r\n",
        "    return interps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  2401\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ1t-LzgpBKF",
        "outputId": "01c47d2d-948e-47dd-f751-4591b1c1fa99"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "    print(device)\r\n",
        "\r\n",
        "    batch_size  = 64\r\n",
        "    img_dim = 96\r\n",
        "    n_channels  = 3\r\n",
        "\r\n",
        "    VQ_lr = 0.001\r\n",
        "    Pixel_lr = 0.0001\r\n",
        "    lr_decay = 0.999995\r\n",
        "\r\n",
        "    dataset = 'stl10'\r\n",
        "    train_loader, len_train, class_names, bird_loader, horse_loader, data_varience = get_data(dataset)\r\n",
        "\r\n",
        "    train_iterator = iter(cycle(train_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFqc1o4zpCAk"
      },
      "source": [
        "bird_iterator = iter(cycle(bird_loader))\r\n",
        "horse_iterator = iter(cycle(horse_loader))\r\n",
        "\r\n",
        "model, epoch, optimiser, scheduler, model_p, optimiser_p, epoch_p = set_up_model()\r\n",
        "train(model, model_p, epoch, optimiser, optimiser_p, epoch_p)\r\n",
        "#interpolate_birds_horses(model)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4oDh8u2pfgZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}